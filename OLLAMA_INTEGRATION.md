# ğŸ¤– ExpertMind + Ollama Integration Guide

## ğŸ“‹ **TL;DR - Quick Setup**

1. **Backend**: `yarn docker:up` o `yarn start` (puerto 3001)
2. **Frontend**: `chmod +x integrate-ollama.sh && ./integrate-ollama.sh`
3. **Test**: `yarn dev:frontend` y configurar en UI

---

## ğŸ” **Problema Identificado**

### Frontend Original:
```json
{
  "message": "texto del usuario",
  "config": { "model": "gpt-3.5-turbo" }
}
```

### Tu Backend Ollama:
```json
{
  "model": "tinyllama",
  "messages": [
    { "role": "user", "content": "texto del usuario" }
  ]
}
```

## ğŸ› ï¸ **Archivos Creados**

### 1. `src/utils/ollama-api.ts`
- **OllamaApiManager**: Clase especÃ­fica para tu backend
- **Formato correcto**: Transforma mensajes al formato Ollama
- **Historial de conversaciÃ³n**: Mantiene contexto entre mensajes
- **Validaciones especÃ­ficas**: Para modelos y configuraciÃ³n Ollama

### 2. `src/App-ollama.tsx`
- **IntegraciÃ³n completa**: Usa OllamaApiManager en lugar de ApiManager genÃ©rico
- **GestiÃ³n de historial**: Sincroniza conversaciones con base de datos
- **Indicador de estado**: Muestra conexiÃ³n con backend y modelos disponibles

### 3. `src/components/ConfigPanel-ollama.tsx`
- **Presets especÃ­ficos**: Configuraciones optimizadas para Ollama
- **Modelos recomendados**: TinyLlama, Gemma2, Phi3, etc.
- **Test de conexiÃ³n**: Verifica backend + Ollama
- **UI optimizada**: Para configuraciones especÃ­ficas de Ollama

---

## ğŸš€ **IntegraciÃ³n AutomÃ¡tica**

```bash
# Ejecutar desde root del proyecto
chmod +x integrate-ollama.sh
./integrate-ollama.sh
```

### QuÃ© hace el script:
1. âœ… Backup de archivos originales
2. âœ… Aplica versiones optimizadas para Ollama
3. âœ… Mantiene compatibilidad con sistema original

---

## ğŸ”— **Curl Commands para Testing**

### **Health Checks**
```bash
# 1. Backend funcionando
curl http://localhost:3001/health

# 2. Estado de Ollama
curl http://localhost:3001/ollama/status

# 3. Modelos disponibles
curl http://localhost:3001/ollama/models
```

### **Chat con Ollama (Como lo usa el frontend)**
```bash
# Mensaje simple
curl -X POST http://localhost:3001/ollama/chat \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "tinyllama",
    "messages": [
      {
        "role": "user",
        "content": "Hola, Â¿puedes explicarme quÃ© es la inteligencia artificial?"
      }
    ]
  }'

# Con configuraciÃ³n personalizada
curl -X POST http://localhost:3001/ollama/chat \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "tinyllama",
    "messages": [
      {
        "role": "system",
        "content": "Eres un asistente experto en tecnologÃ­a."
      },
      {
        "role": "user",
        "content": "Â¿CuÃ¡les son las ventajas de usar Docker?"
      }
    ],
    "options": {
      "temperature": 0.7,
      "num_ctx": 2048
    }
  }'

# ConversaciÃ³n con contexto
curl -X POST http://localhost:3001/ollama/chat \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "tinyllama",
    "messages": [
      {
        "role": "user",
        "content": "Â¿QuÃ© es React?"
      },
      {
        "role": "assistant", 
        "content": "React es una biblioteca de JavaScript para construir interfaces de usuario..."
      },
      {
        "role": "user",
        "content": "Â¿Y quÃ© ventajas tiene sobre Vue?"
      }
    ]
  }'
```

### **Generate (Respuesta simple)**
```bash
# Para completar texto
curl -X POST http://localhost:3001/ollama/generate \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "tinyllama",
    "prompt": "Explica en pocas palabras quÃ© es React.js"
  }'
```

### **GestiÃ³n de modelos**
```bash
# Descargar TinyLlama
curl -X POST http://localhost:3001/ollama/pull/tinyllama

# Descargar Gemma 2B
curl -X POST http://localhost:3001/ollama/pull/gemma2:2b
```

---

## ğŸ“± **ConfiguraciÃ³n en la UI**

### 1. **Abrir panel de configuraciÃ³n**
- Click en âš™ï¸ en el sidebar

### 2. **Usar preset "Local Development"**
- URL: `http://localhost:3001`
- Modelo: `tinyllama`
- Temperature: `0.7`

### 3. **Test conexiÃ³n**
- Click "ğŸ”— Test Connection"
- Click "ğŸ“‹ Get Models" para ver modelos disponibles

### 4. **Guardar configuraciÃ³n**
- Click "ğŸ’¾ Save Configuration"

---

## ğŸ”§ **Flujo de Datos Completo**

```
Usuario escribe mensaje
    â†“
MessageInput.tsx â†’ onSendMessage()
    â†“
App.tsx â†’ sendMessage()
    â†“
OllamaApiManager.sendMessage()
    â†“
POST http://localhost:3001/ollama/chat
    â†“
NestJS Backend â†’ OllamaController.chat()
    â†“
OllamaService.chat()
    â†“
POST http://localhost:11434/api/chat (Ollama)
    â†“
Respuesta de Ollama
    â†“
Backend â†’ Frontend â†’ UI
```

---

## ğŸ› **Troubleshooting**

### **Error: EADDRINUSE port 3001**
```bash
yarn docker:down  # Detener Docker
yarn start        # O ejecutar localmente
```

### **Error: Ollama not ready**
```bash
# Verificar Docker
docker ps | grep ollama

# O instalar Ollama localmente
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve
ollama pull tinyllama
```

### **Frontend no conecta**
1. âœ… Backend en puerto 3001: `curl http://localhost:3001/health`
2. âœ… Ollama funcionando: `curl http://localhost:3001/ollama/status`
3. âœ… ConfiguraciÃ³n UI: URL = `http://localhost:3001`

### **Error de CORS**
- Ya estÃ¡ configurado en `main.ts` del backend
- Permite `http://localhost:5173` (Vite) y `http://localhost:3000`

---

## ğŸ¯ **Modelos Recomendados**

| Modelo | TamaÃ±o | Velocidad | Uso |
|--------|--------|-----------|-----|
| `tinyllama` | 1.1B | ğŸš€ğŸš€ğŸš€ | Desarrollo rÃ¡pido |
| `gemma2:2b` | 2B | ğŸš€ğŸš€ | Balanceado |
| `phi3:mini` | 3.8B | ğŸš€ | Mejor calidad |
| `llama3.2:3b` | 3B | ğŸš€ | Multilenguaje |
| `codellama:7b` | 7B | ğŸŒ | ProgramaciÃ³n |

---

## ğŸ”„ **Revertir Cambios**

```bash
# Volver a la configuraciÃ³n original
cp apps/frontend/src/App-original.tsx apps/frontend/src/App.tsx
cp apps/frontend/src/components/ConfigPanel-original.tsx apps/frontend/src/components/ConfigPanel.tsx
```

---

## ğŸ‰ **Â¡Listo para usar!**

Ahora tu frontend estÃ¡ completamente integrado con tu backend de Ollama. El chatbox enviarÃ¡ mensajes en el formato correcto y mantendrÃ¡ el historial de conversaciÃ³n.

**Â¿Funciona todo?** Prueba enviando: "Hola, explÃ­came quÃ© es JavaScript"
