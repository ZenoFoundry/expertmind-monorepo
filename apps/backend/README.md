# ExpertMind Backend

Backend desarrollado en NestJS con integraciÃ³n a Ollama para el proyecto ExpertMind.

## ğŸš€ CaracterÃ­sticas

- API REST desarrollada con NestJS
- IntegraciÃ³n completa con Ollama
- Soporte para mÃºltiples modelos de IA
- DocumentaciÃ³n automÃ¡tica con Swagger
- ConfiguraciÃ³n Docker lista para producciÃ³n
- ValidaciÃ³n de datos con class-validator
- Logs estructurados

## ğŸ“‹ Prerrequisitos

- Node.js 18 o superior
- Yarn
- Docker y Docker Compose (para desarrollo con contenedores)

## ğŸ› ï¸ InstalaciÃ³n y ConfiguraciÃ³n

### Desarrollo Local

1. **Instalar dependencias:**
   ```bash
   yarn install
   ```

2. **Configurar variables de entorno:**
   ```bash
   cp .env.example .env
   ```

3. **Ejecutar en modo desarrollo:**
   ```bash
   yarn start:dev
   ```

### Desarrollo con Docker

1. **Desde la raÃ­z del monorepo, ejecutar:**
   ```bash
   yarn docker:up
   ```

   Esto iniciarÃ¡:
   - Backend en `http://localhost:3001`
   - Ollama en `http://localhost:11434`
   - Frontend en `http://localhost:5173`
   - Descarga automÃ¡tica de TinyLlama

2. **Ver logs:**
   ```bash
   # Todos los servicios
   yarn docker:logs
   
   # Solo backend
   yarn docker:logs:backend
   
   # Solo Ollama
   yarn docker:logs:ollama
   ```

## ğŸ“š API Endpoints

### Estado del Servicio
- `GET /` - Health check bÃ¡sico
- `GET /health` - Estado detallado del servicio
- `GET /api` - DocumentaciÃ³n Swagger

### Ollama
- `GET /ollama/models` - Listar modelos disponibles
- `GET /ollama/status` - Estado de conexiÃ³n con Ollama
- `POST /ollama/chat` - ConversaciÃ³n con un modelo
- `POST /ollama/generate` - Generar respuesta con prompt
- `POST /ollama/pull/:modelName` - Descargar un modelo especÃ­fico

## ğŸ’¡ Ejemplos de Uso

### Chat con TinyLlama

```bash
curl -X POST http://localhost:3001/ollama/chat \\
  -H \"Content-Type: application/json\" \\
  -d '{
    \"model\": \"tinyllama\",
    \"messages\": [
      {
        \"role\": \"user\",
        \"content\": \"Â¿QuÃ© es la inteligencia artificial?\"
      }
    ]
  }'
```

### Generar respuesta simple

```bash
curl -X POST http://localhost:3001/ollama/generate \\
  -H \"Content-Type: application/json\" \\
  -d '{
    \"model\": \"tinyllama\",
    \"prompt\": \"Explica brevemente quÃ© es Node.js\"
  }'
```

### Verificar modelos disponibles

```bash
curl http://localhost:3001/ollama/models
```

## ğŸ”§ ConfiguraciÃ³n

### Variables de Entorno

| Variable | DescripciÃ³n | Valor por defecto |
|----------|-------------|-------------------|
| `PORT` | Puerto del servidor | `3001` |
| `NODE_ENV` | Entorno de ejecuciÃ³n | `development` |
| `OLLAMA_URL` | URL de Ollama | `http://localhost:11434` |

### Modelos Soportados

El sistema viene preconfigurado con TinyLlama, pero puedes usar cualquier modelo compatible con Ollama:

- `tinyllama` (incluido por defecto)
- `llama2`
- `codellama`
- `mistral`
- Y muchos mÃ¡s...

## ğŸ§ª Testing

```bash
# Tests unitarios
yarn test

# Tests con coverage
yarn test:cov

# Tests e2e
yarn test:e2e
```

## ğŸ“– DocumentaciÃ³n

Una vez que el servidor estÃ© corriendo, la documentaciÃ³n interactiva estarÃ¡ disponible en:
- **Swagger UI:** http://localhost:3001/api

## ğŸ› Troubleshooting

### Ollama no se conecta

1. Verificar que Ollama estÃ© corriendo:
   ```bash
   curl http://localhost:11434/api/tags
   ```

2. Revisar logs de Ollama:
   ```bash
   yarn docker:logs:ollama
   ```

### Modelo no encontrado

1. Listar modelos disponibles:
   ```bash
   curl http://localhost:3001/ollama/models
   ```

2. Descargar un modelo especÃ­fico:
   ```bash
   curl -X POST http://localhost:3001/ollama/pull/tinyllama
   ```

### Puerto en uso

Si el puerto 3001 estÃ¡ ocupado, puedes cambiarlo en el archivo `.env`:
```env
PORT=3002
```

## ğŸ“ Estructura del Proyecto

```
apps/backend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ollama/           # MÃ³dulo de Ollama
â”‚   â”‚   â”œâ”€â”€ dto/          # Data Transfer Objects
â”‚   â”‚   â”œâ”€â”€ ollama.controller.ts
â”‚   â”‚   â”œâ”€â”€ ollama.service.ts
â”‚   â”‚   â””â”€â”€ ollama.module.ts
â”‚   â”œâ”€â”€ app.controller.ts
â”‚   â”œâ”€â”€ app.service.ts
â”‚   â”œâ”€â”€ app.module.ts
â”‚   â””â”€â”€ main.ts
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ğŸ¤ ContribuciÃ³n

1. Fork el proyecto
2. Crear una rama para tu feature (`git checkout -b feature/nueva-caracteristica`)
3. Commit tus cambios (`git commit -m 'Agregar nueva caracterÃ­stica'`)
4. Push a la rama (`git push origin feature/nueva-caracteristica`)
5. Crear un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT.
