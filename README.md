# ğŸ¤– ExpertMind - AplicaciÃ³n Web de Chat con IA

> **AplicaciÃ³n web completa de chatbox con inteligencia artificial, desarrollada con React y NestJS, integrada con Ollama para modelos de IA locales.**

![ExpertMind](https://img.shields.io/badge/ExpertMind-v1.0.0-blue.svg)
![React](https://img.shields.io/badge/React-18.2.0-61dafb.svg)
![NestJS](https://img.shields.io/badge/NestJS-10.0.0-e0234e.svg)
![TypeScript](https://img.shields.io/badge/TypeScript-5.2.2-3178c6.svg)
![Docker](https://img.shields.io/badge/Docker-Ready-2496ed.svg)

## ğŸ“– DescripciÃ³n

ExpertMind es una aplicaciÃ³n web moderna que permite chatear con modelos de inteligencia artificial de manera local y privada. Utiliza Ollama para ejecutar modelos como TinyLlama sin necesidad de conexiÃ³n a internet, garantizando la privacidad de tus conversaciones.

### âœ¨ CaracterÃ­sticas Principales

- ğŸ¤– **Chat con IA Local** - Conversaciones con modelos de IA ejecutÃ¡ndose localmente
- ğŸ  **100% Privado** - Todos los datos permanecen en tu mÃ¡quina
- ğŸ“± **Interfaz Moderna** - UI responsiva con modo oscuro
- ğŸ“ **Soporte de Archivos** - Subida de imÃ¡genes y documentos
- ğŸ’¾ **Almacenamiento Local** - Historial de conversaciones en localStorage
- ğŸ”„ **MÃºltiples Sesiones** - GestiÃ³n de mÃºltiples conversaciones
- ğŸ³ **Docker Ready** - Setup completo con contenedores
- ğŸ“š **API Documentada** - Swagger para desarrollo
- âš¡ **Desarrollo RÃ¡pido** - Hot reload en frontend y backend

## ğŸ—ï¸ Arquitectura

```mermaid
graph TB
    subgraph "Frontend (Puerto 5173)"
        A[React App]
        B[Vite Dev Server]
    end
    
    subgraph "Backend (Puerto 3001)"
        C[NestJS API]
        D[Swagger Docs]
    end
    
    subgraph "IA (Puerto 11434)"
        E[Ollama Server]
        F[TinyLlama Model]
    end
    
    A --> C
    C --> E
    B -.-> A
    D -.-> C
    F -.-> E
```

### ğŸ› ï¸ Stack TecnolÃ³gico

| Componente | TecnologÃ­a | DescripciÃ³n |
|------------|------------|-------------|
| **Frontend** | React 18 + TypeScript | Interfaz de usuario moderna y reactiva |
| **Build Tool** | Vite | Desarrollo rÃ¡pido con hot reload |
| **Backend** | NestJS + TypeScript | API REST robusta y escalable |
| **IA Engine** | Ollama + TinyLlama | Procesamiento de IA local |
| **Almacenamiento** | localStorage | Persistencia de datos en navegador |
| **Contenedores** | Docker Compose | OrquestaciÃ³n completa de servicios |
| **DocumentaciÃ³n** | Swagger/OpenAPI | API documentada automÃ¡ticamente |

## ğŸš€ Inicio RÃ¡pido

### Prerrequisitos

```bash
# Verificar versiones requeridas
node --version  # >= 18.0.0
yarn --version  # >= 1.22.0
docker --version
docker-compose --version
```

### InstalaciÃ³n con Docker (Recomendado)

```bash
# 1. Clonar el repositorio
git clone <url-del-repositorio>
cd expertmind-monorepo

# 2. Instalar dependencias
yarn install

# 3. Levantar todos los servicios
yarn dev

# 4. Esperar a que descargue TinyLlama (3-5 minutos la primera vez)
yarn docker:logs:ollama

# 5. Verificar que todo estÃ© funcionando
curl http://localhost:3001/health
```

### URLs de la AplicaciÃ³n

| Servicio | URL | DescripciÃ³n |
|----------|-----|-------------|
| ğŸŒ **Frontend** | http://localhost:5173 | Interfaz principal de la aplicaciÃ³n |
| ğŸ”§ **Backend API** | http://localhost:3001 | API REST del backend |
| ğŸ“š **DocumentaciÃ³n** | http://localhost:3001/api | Swagger UI con documentaciÃ³n interactiva |
| ğŸ¤– **Ollama** | http://localhost:11434 | Servidor de IA (solo API) |

## ğŸ’» Desarrollo Local

### OpciÃ³n 1: Todo con Docker
```bash
yarn dev  # Levanta frontend + backend + ollama
```

### OpciÃ³n 2: Desarrollo HÃ­brido
```bash
# Terminal 1: Servicios base
yarn docker:up

# Terminal 2: Backend en desarrollo
yarn dev:backend

# Terminal 3: Frontend en desarrollo  
yarn dev:frontend
```

### OpciÃ³n 3: Completamente Local
```bash
# Requisito: Ollama instalado localmente
# Ver: https://ollama.ai/download

# Terminal 1: Ollama
ollama serve
ollama pull tinyllama

# Terminal 2: Backend
cd apps/backend
yarn install
yarn start:dev

# Terminal 3: Frontend
cd apps/frontend
yarn install
yarn dev
```

## ğŸ“‹ Scripts Disponibles

### Scripts Principales
```bash
# Desarrollo con Docker
yarn dev                    # Levanta todos los servicios
yarn dev:local             # Frontend + Backend local (sin Docker)

# Desarrollo individual
yarn dev:frontend          # Solo frontend (puerto 5173)
yarn dev:backend           # Solo backend (puerto 3001)

# Build
yarn build                 # Construye todos los workspaces
yarn build:frontend        # Solo frontend
yarn build:backend         # Solo backend

# Testing
yarn test                  # Todas las pruebas
yarn lint                  # AnÃ¡lisis de cÃ³digo
yarn type-check            # VerificaciÃ³n de tipos TypeScript
```

### Scripts de Docker
```bash
# GestiÃ³n de contenedores
yarn docker:up             # Levantar servicios
yarn docker:down           # Detener servicios
yarn docker:logs           # Ver todos los logs
yarn docker:logs:backend   # Solo logs del backend
yarn docker:logs:ollama    # Solo logs de Ollama

# Limpieza
yarn clean                 # Limpia archivos build y node_modules
```

## ğŸ“ Estructura del Proyecto

```
expertmind-monorepo/
â”œâ”€â”€ ğŸ“ apps/
â”‚   â”œâ”€â”€ ğŸ“ frontend/                 # AplicaciÃ³n React
â”‚   â”‚   â”œâ”€â”€ ğŸ“ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ components/       # Componentes React
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ChatArea.tsx
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ MessageInput.tsx
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ MessageList.tsx
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Sidebar.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ utils/            # Utilidades
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ api.ts           # GestiÃ³n de API
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ database.ts      # localStorage manager
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ fileUtils.ts     # Manejo de archivos
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ types/            # Tipos TypeScript
â”‚   â”‚   â”‚   â””â”€â”€ App.tsx              # Componente principal
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ package.json
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ vite.config.ts
â”‚   â”‚   â””â”€â”€ ğŸ“„ Dockerfile
â”‚   â””â”€â”€ ğŸ“ backend/                  # API NestJS
â”‚       â”œâ”€â”€ ğŸ“ src/
â”‚       â”‚   â”œâ”€â”€ ğŸ“ ollama/           # MÃ³dulo Ollama
â”‚       â”‚   â”‚   â”œâ”€â”€ ğŸ“ dto/          # Data Transfer Objects
â”‚       â”‚   â”‚   â”œâ”€â”€ ollama.controller.ts
â”‚       â”‚   â”‚   â”œâ”€â”€ ollama.service.ts
â”‚       â”‚   â”‚   â””â”€â”€ ollama.module.ts
â”‚       â”‚   â”œâ”€â”€ app.module.ts        # MÃ³dulo principal
â”‚       â”‚   â””â”€â”€ main.ts              # Punto de entrada
â”‚       â”œâ”€â”€ ğŸ“ scripts/              # Scripts utiles
â”‚       â”‚   â””â”€â”€ test-ollama.sh       # Pruebas de conexiÃ³n
â”‚       â”œâ”€â”€ ğŸ“„ package.json
â”‚       â”œâ”€â”€ ğŸ“„ Dockerfile
â”‚       â””â”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“„ docker-compose.yml           # OrquestaciÃ³n completa
â”œâ”€â”€ ğŸ“„ package.json                 # Workspace root
â””â”€â”€ ğŸ“„ README.md                    # Este archivo
```

## ğŸ¤– API del Backend

### Endpoints Principales

#### Health & Status
```bash
# Estado general del servicio
GET /health
GET /

# Estado de conexiÃ³n con Ollama
GET /ollama/status
```

#### GestiÃ³n de Modelos
```bash
# Listar modelos disponibles
GET /ollama/models

# Descargar un modelo especÃ­fico
POST /ollama/pull/:modelName
```

#### InteracciÃ³n con IA
```bash
# Chat conversacional
POST /ollama/chat
Content-Type: application/json
{
  "model": "tinyllama",
  "messages": [
    {"role": "user", "content": "Hola, Â¿cÃ³mo estÃ¡s?"}
  ],
  "options": {
    "temperature": 0.7
  }
}

# GeneraciÃ³n de respuesta simple
POST /ollama/generate
Content-Type: application/json
{
  "model": "tinyllama",
  "prompt": "Explica quÃ© es JavaScript",
  "options": {
    "temperature": 0.8
  }
}
```

### Ejemplos de Uso con curl

```bash
# Verificar que el backend estÃ¡ funcionando
curl http://localhost:3001/health

# Ver modelos disponibles
curl http://localhost:3001/ollama/models

# Chat simple
curl -X POST http://localhost:3001/ollama/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tinyllama",
    "messages": [
      {"role": "user", "content": "Â¿QuÃ© es Node.js?"}
    ]
  }'

# Generar respuesta
curl -X POST http://localhost:3001/ollama/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tinyllama",
    "prompt": "Escribe un haiku sobre programaciÃ³n"
  }'

# Descargar modelo adicional
curl -X POST http://localhost:3001/ollama/pull/llama2
```

## ğŸ³ Docker Setup

### Servicios Incluidos

| Servicio | Puerto | DescripciÃ³n |
|----------|--------|-------------|
| **frontend** | 5173 | AplicaciÃ³n React con Vite |
| **backend** | 3001 | API NestJS |
| **ollama** | 11434 | Servidor Ollama |
| **ollama-setup** | - | Descarga automÃ¡tica de TinyLlama |

### Docker Compose

```yaml
# Ver configuraciÃ³n completa en docker-compose.yml
services:
  frontend:    # React app
  backend:     # NestJS API  
  ollama:      # Ollama server
  ollama-setup: # Auto-descarga TinyLlama
```

### Comandos Docker Ãštiles

```bash
# Rebuild completo desde cero
docker-compose down -v --remove-orphans
docker-compose build --no-cache
docker-compose up -d

# Ver recursos utilizados
docker-compose ps
docker-compose top

# Ejecutar comandos en contenedores
docker-compose exec backend yarn test
docker-compose exec frontend yarn build
docker-compose exec ollama ollama list

# Limpiar sistema Docker
docker system prune -a
docker volume prune
```

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno

#### Frontend (`.env`)
```env
# URL del backend API
VITE_API_URL=http://localhost:3001

# Puerto de desarrollo (opcional)
VITE_PORT=5173
VITE_HOST=localhost
```

#### Backend (`.env`)
```env
# ConfiguraciÃ³n del servidor
PORT=3001
NODE_ENV=development

# URL de Ollama
OLLAMA_URL=http://localhost:11434
# Para Docker: OLLAMA_URL=http://ollama:11434
```

### Modelos de IA Soportados

Por defecto se incluye **TinyLlama**, pero puedes usar cualquier modelo compatible con Ollama:

| Modelo | TamaÃ±o | DescripciÃ³n |
|--------|--------|-------------|
| `tinyllama` | ~637MB | Modelo ligero incluido por defecto |
| `llama2` | ~3.8GB | Modelo mÃ¡s potente de Meta |
| `codellama` | ~3.8GB | Especializado en cÃ³digo |
| `mistral` | ~4.1GB | Modelo eficiente y rÃ¡pido |
| `phi` | ~1.6GB | Modelo pequeÃ±o de Microsoft |

```bash
# Descargar modelo adicional
curl -X POST http://localhost:3001/ollama/pull/llama2

# O directamente con Ollama
docker-compose exec ollama ollama pull llama2
```

## ğŸ§ª Testing

### Pruebas Automatizadas
```bash
# Todas las pruebas
yarn test

# Por workspace
yarn workspace @expertmind/frontend test
yarn workspace @expertmind/backend test

# Con coverage
yarn workspace @expertmind/backend test:cov
```

### Pruebas Manuales
```bash
# Script de pruebas completas del backend + Ollama
chmod +x apps/backend/scripts/test-ollama.sh
./apps/backend/scripts/test-ollama.sh

# Pruebas individuales
curl http://localhost:3001/health
curl http://localhost:3001/ollama/status
curl http://localhost:3001/ollama/models
```

## ğŸ”§ Desarrollo

### Hot Reload
- âœ… **Frontend**: Cambios en tiempo real con Vite
- âœ… **Backend**: Auto-restart con NestJS watch mode
- âœ… **Types**: VerificaciÃ³n automÃ¡tica de TypeScript

### Debugging
```bash
# Backend con debugging
yarn workspace @expertmind/backend start:debug

# Frontend con sourcemaps habilitados (automÃ¡tico)
yarn dev:frontend

# Ver logs en tiempo real
yarn docker:logs -f
```

### Linting y Formatting
```bash
# Verificar cÃ³digo
yarn lint

# Auto-fix problemas de linting
yarn workspace @expertmind/frontend lint --fix
yarn workspace @expertmind/backend lint --fix

# Formato con Prettier (si estÃ¡ configurado)
yarn format
```

## ğŸ“Š Monitoreo

### Logs
```bash
# Todos los servicios
yarn docker:logs

# Servicio especÃ­fico
yarn docker:logs:backend
yarn docker:logs:ollama
yarn docker:logs frontend

# Seguir logs en tiempo real
yarn docker:logs -f
```

### Health Checks
```bash
# Estado general
curl http://localhost:3001/health

# Estado de Ollama
curl http://localhost:3001/ollama/status

# MÃ©tricas del sistema
curl http://localhost:11434/api/tags
```

## ğŸš¨ Troubleshooting

### Problemas Comunes

#### ğŸ”´ Ollama no responde
```bash
# Verificar estado
curl http://localhost:11434/api/tags

# Reiniciar servicio
docker-compose restart ollama

# Ver logs
yarn docker:logs:ollama

# Verificar que TinyLlama estÃ© descargado
docker-compose exec ollama ollama list
```

#### ğŸ”´ Puerto en uso
```bash
# Verificar quÃ© estÃ¡ usando el puerto
lsof -ti:5173 | xargs kill -9  # Frontend
lsof -ti:3001 | xargs kill -9  # Backend
lsof -ti:11434 | xargs kill -9 # Ollama

# O cambiar puertos en docker-compose.yml
```

#### ğŸ”´ Frontend no se conecta al backend
```bash
# Verificar que el backend estÃ© corriendo
curl http://localhost:3001/health

# Verificar variable de entorno
echo $VITE_API_URL

# Ver logs del navegador
# Abrir DevTools > Console
```

#### ğŸ”´ Problemas de memoria
```bash
# Verificar uso de Docker
docker stats

# Incrementar memoria asignada a Docker
# Docker Desktop > Settings > Resources > Memory
```

#### ğŸ”´ Build falla
```bash
# Limpiar todo y reconstruir
yarn clean
rm -rf node_modules apps/*/node_modules
yarn install

# Docker
docker-compose down -v
docker-compose build --no-cache
docker-compose up -d
```

### Logs de DepuraciÃ³n

```bash
# Backend detallado
NODE_ENV=development yarn dev:backend

# Frontend con info detallada
VITE_LOG_LEVEL=info yarn dev:frontend

# Ollama con debug
docker-compose exec ollama ollama serve --debug
```

## ğŸš€ Deployment

### Build para ProducciÃ³n
```bash
# Build completo
yarn build

# Solo frontend
yarn build:frontend
# Archivos en: apps/frontend/dist/

# Solo backend
yarn build:backend  
# Archivos en: apps/backend/dist/
```

### Docker en ProducciÃ³n
```bash
# Variables de entorno para producciÃ³n
export NODE_ENV=production
export VITE_API_URL=https://tu-api.com

# Build y deploy
docker-compose -f docker-compose.prod.yml up -d
```

### Hosting Sugerido
- **Frontend**: Vercel, Netlify, GitHub Pages
- **Backend**: Railway, Render, DigitalOcean
- **Todo Junto**: VPS con Docker Compose

## ğŸ¤ ContribuciÃ³n

### Setup para Desarrollo
```bash
# 1. Fork del repositorio
# 2. Clonar tu fork
git clone https://github.com/tu-usuario/expertmind-monorepo.git

# 3. Instalar dependencias
yarn install

# 4. Crear rama feature
git checkout -b feature/nueva-funcionalidad

# 5. Desarrollar y probar
yarn dev
yarn test

# 6. Commit y push
git commit -m "feat: agregar nueva funcionalidad"
git push origin feature/nueva-funcionalidad

# 7. Crear Pull Request
```

### Convenciones
- ğŸ“ **Commits**: Usar [Conventional Commits](https://conventionalcommits.org/)
- ğŸ·ï¸ **Branches**: `feature/`, `bugfix/`, `hotfix/`
- ğŸ§ª **Testing**: Incluir pruebas para nueva funcionalidad
- ğŸ“š **Docs**: Actualizar documentaciÃ³n si es necesario

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la **Licencia MIT** - ver el archivo [LICENSE](LICENSE) para detalles.

## ğŸ™ Agradecimientos

- [Ollama](https://ollama.ai/) - Por hacer la IA local accesible
- [NestJS](https://nestjs.com/) - Framework backend robusto
- [React](https://react.dev/) - LibrerÃ­a frontend moderna
- [Vite](https://vitejs.dev/) - Build tool ultrarrÃ¡pido

---

<div align="center">

**â­ Si te gusta el proyecto, Â¡dale una estrella en GitHub! â­**

![Made with â¤ï¸](https://img.shields.io/badge/Made%20with-â¤ï¸-red.svg)
![Open Source](https://img.shields.io/badge/Open%20Source-ğŸ’š-green.svg)

</div>
