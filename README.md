# ğŸš€ ExpertMind - AplicaciÃ³n Web de Chatbox con IA

AplicaciÃ³n web moderna desarrollada como monorepo con React (frontend) y NestJS (backend), integrada con Ollama para capacidades de inteligencia artificial.

## ğŸ—ï¸ Arquitectura

- **Frontend:** React + TypeScript + Vite
- **Backend:** NestJS + TypeScript
- **IA:** Ollama con TinyLlama
- **Contenedores:** Docker + Docker Compose
- **GestiÃ³n:** Yarn Workspaces

## ğŸ“‹ Requisitos Previos

- **Node.js** (versiÃ³n 18 o superior)
- **Yarn** (gestor de paquetes)
- **Docker** y **Docker Compose** (para desarrollo con contenedores)

## ğŸš€ Inicio RÃ¡pido

### OpciÃ³n 1: Con Docker (Recomendado)

```bash
# Clonar el repositorio
git clone <url-del-repositorio>
cd expertmind-monorepo

# Instalar dependencias
yarn install

# Levantar todos los servicios
yarn dev

# Esperar a que descargue TinyLlama (unos minutos la primera vez)
yarn docker:logs:ollama
```

**URLs disponibles:**
- ğŸŒ Frontend: http://localhost:5173
- ğŸ”§ Backend API: http://localhost:3001
- ğŸ“š DocumentaciÃ³n API: http://localhost:3001/api
- ğŸ¤– Ollama: http://localhost:11434

### OpciÃ³n 2: Desarrollo Local

```bash
# Instalar dependencias
yarn install

# En una terminal: iniciar backend
yarn dev:backend

# En otra terminal: iniciar frontend
yarn dev:frontend

# Nota: NecesitarÃ¡s Ollama corriendo por separado
```

## ğŸ› ï¸ Scripts Disponibles

### Scripts Principales
| Comando | DescripciÃ³n |
|---------|-------------|
| `yarn dev` | Levanta todos los servicios con Docker |
| `yarn dev:local` | Desarrollo local (frontend + backend) |
| `yarn build` | Construye todos los workspaces |
| `yarn test` | Ejecuta pruebas en todos los workspaces |
| `yarn clean` | Limpia archivos generados |

### Scripts de Docker
| Comando | DescripciÃ³n |
|---------|-------------|
| `yarn docker:up` | Levanta contenedores |
| `yarn docker:down` | Detiene contenedores |
| `yarn docker:logs` | Logs de todos los servicios |
| `yarn docker:logs:backend` | Logs solo del backend |
| `yarn docker:logs:ollama` | Logs solo de Ollama |

### Scripts por Workspace
| Comando | DescripciÃ³n |
|---------|-------------|
| `yarn dev:frontend` | Solo frontend |
| `yarn dev:backend` | Solo backend |
| `yarn build:frontend` | Build frontend |
| `yarn build:backend` | Build backend |

## ğŸ“ Estructura del Proyecto

```
expertmind-monorepo/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ frontend/         # React + TypeScript + Vite
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ package.json
â”‚   â”‚   â””â”€â”€ vite.config.ts
â”‚   â””â”€â”€ backend/          # NestJS + TypeScript
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ ollama/   # MÃ³dulo Ollama
â”‚       â”‚   â””â”€â”€ main.ts
â”‚       â”œâ”€â”€ package.json
â”‚       â””â”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml    # ConfiguraciÃ³n completa
â”œâ”€â”€ package.json          # Workspace root
â””â”€â”€ README.md            # Este archivo
```

## ğŸ¤– API de Ollama

### Endpoints Principales

```bash
# Estado de la API
curl http://localhost:3001/health

# Modelos disponibles
curl http://localhost:3001/ollama/models

# Chat con IA
curl -X POST http://localhost:3001/ollama/chat \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "tinyllama",
    "messages": [{"role": "user", "content": "Hola!"}]
  }'

# Generar respuesta
curl -X POST http://localhost:3001/ollama/generate \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "tinyllama",
    "prompt": "Explica quÃ© es JavaScript"
  }'
```

## ğŸ³ Docker

### Comandos Ãštiles

```bash
# Rebuild completo
docker-compose down
docker-compose build --no-cache
docker-compose up -d

# Ver logs en tiempo real
docker-compose logs -f

# Entrar a un contenedor
docker-compose exec backend bash
docker-compose exec ollama bash

# Limpiar todo
docker-compose down -v --remove-orphans
docker system prune -a
```

### Servicios Incluidos

- **frontend** - React app en puerto 5173
- **backend** - NestJS API en puerto 3001  
- **ollama** - Servidor Ollama en puerto 11434
- **ollama-setup** - Descarga automÃ¡tica de TinyLlama

## ğŸ”§ ConfiguraciÃ³n

### Variables de Entorno

#### Frontend (.env)
```env
VITE_API_URL=http://localhost:3001
```

#### Backend (.env)
```env
PORT=3001
NODE_ENV=development
OLLAMA_URL=http://localhost:11434
```

## ğŸ§ª Testing

```bash
# Todas las pruebas
yarn test

# Solo frontend
yarn workspace @expertmind/frontend test

# Solo backend
yarn workspace @expertmind/backend test

# Probar conexiÃ³n con Ollama
./apps/backend/scripts/test-ollama.sh
```

## ğŸ“š DocumentaciÃ³n

- **API Backend:** http://localhost:3001/api (Swagger)
- **Frontend:** Ver `apps/frontend/README.md`
- **Backend:** Ver `apps/backend/README.md`

## ğŸš¨ Troubleshooting

### Ollama no responde
```bash
# Verificar estado
curl http://localhost:11434/api/tags

# Revisar logs
yarn docker:logs:ollama

# Reiniciar servicio
docker-compose restart ollama
```

### Puerto en uso
```bash
# Cambiar puertos en docker-compose.yml
# O detener otros servicios
lsof -ti:5173 | xargs kill -9
lsof -ti:3001 | xargs kill -9
```

### Problemas de build
```bash
# Limpiar y rebuildar
yarn clean
yarn install
docker-compose down -v
docker-compose build --no-cache
```

## ğŸ¤ ContribuciÃ³n

1. Fork el proyecto
2. Crear rama feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -m 'Agregar nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crear Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo LICENSE para detalles.

---

Â¡Tu aplicaciÃ³n web con IA estÃ¡ lista para usar! ğŸ‰
